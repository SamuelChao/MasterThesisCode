{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = input(\"Please enter the directory: \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir + 'Labeled_SERS_dataset.csv', header = 0)\n",
    "# Sorting by Label\n",
    "df.sort_values(by = 'Label', inplace=True)\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, 400.0:1550.0]\n",
    "xlabel_General = np.array([ 'BW25113','BW25113_Uninhibited','BW25113_Inhibited','DH5\\u03B1', 'DH5\\u03B1_Uninhibited', 'DH5\\u03B1_Inhibited', 'DH5\\u03B1(ampR)_Uninhibited'])\n",
    "x_General = np.array([0, 1, 2, 3, 4, 5, 6])\n",
    "color_map = [ 'powderblue', 'steelblue', 'navy', '#F6BE00','firebrick', 'maroon',  'olivedrab']\n",
    "marker_list = ['v', 'v', 'v',  'o', '8', '8', 'p', ]\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_feature = pca.fit_transform(feature)\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(pca_feature.shape[0]):  \n",
    "    if temp_label == 0 and int(df.iloc[i, 2301]) == 0:\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 2301])], label = xlabel_General[int(df.iloc[i, 2301])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 2301])], edgecolors = None)\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 2301]) and (temp_label != 0):\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 2301])], label = xlabel_General[int(df.iloc[i, 2301])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 2301])], edgecolors = None)\n",
    "        temp_label = temp_label + 1\n",
    "        print( xlabel_General[int(df.iloc[i, 2301])])\n",
    "        print( int(df.iloc[i, 2301]) )\n",
    "    else :\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 2301])], alpha = 0.5, s = 7, marker = marker_list[int(df.iloc[i, 2301])], edgecolors = None)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.axis('square')\n",
    "plt.savefig(input_dir + 'Labeled_SERS_PCA.png', dpi=300, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir + 'Labeled_SERS_dataset.csv', header = 0)\n",
    "# Sorting by Label\n",
    "df.sort_values(by = 'Label', inplace=True)\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, 400.0:1550.0]\n",
    "xlabel_General = np.array([ 'BW25113','BW25113_Uninhibited','BW25113_Inhibited','DH5\\u03B1', 'DH5\\u03B1_Uninhibited', 'DH5\\u03B1_Inhibited', 'DH5\\u03B1(ampR)_Uninhibited'])\n",
    "x_General = np.array([0, 1, 2, 3, 4, 5, 6])\n",
    "color_map = [ 'powderblue', 'steelblue', 'navy', '#F6BE00','firebrick', 'maroon',  'olivedrab']\n",
    "marker_list = ['v', 'v', 'v',  'o', '8', '8', 'p', ]\n",
    "\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate=200, perplexity = 50).fit_transform(feature)\n",
    "x_min, x_max = tsne.min(0), tsne.max(0)\n",
    "tsne = (tsne - x_min) / (x_max - x_min)\n",
    "\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(tsne.shape[0]):\n",
    "    if temp_label == 0 and int(df.iloc[i, 2301]) == 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 2301])], label = xlabel_General[int(df.iloc[i, 2301])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 2301])])\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 2301]) and temp_label != 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 2301])], label = xlabel_General[int(df.iloc[i, 2301])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 2301])])\n",
    "        temp_label = temp_label + 1\n",
    "    else :\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 2301])], alpha = 0.5, s=10,marker = marker_list[int(df.iloc[i, 2301])])\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "plt.xlabel('T-SNE1')\n",
    "plt.ylabel('T-SNE2')\n",
    "plt.axis('square')\n",
    "plt.savefig(input_dir + 'Labeled_SERS_t-SNE.png', dpi=300, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine learning classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML training: RF, SVM, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir + 'SERS_training.csv',header = 0)\n",
    "feature = df.loc[:, 400.0:1550.0]\n",
    "train_label = df['Label'].to_numpy()\n",
    "\n",
    "## Model Training\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(max_depth=40, max_samples=1.0, min_samples_split=5,random_state=0)\n",
    "rf.fit(feature, train_label)\n",
    "\n",
    "#SVM\n",
    "svm = SVC(C=10, kernel='linear')\n",
    "svm.fit(feature, train_label)\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(algorithm='brute', n_neighbors=10, weights='distance')\n",
    "knn.fit(feature, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = 200\n",
    "BATCH_SIZE = 200\n",
    "learning_rate = 0.0001 \n",
    "wd=0.00001\n",
    "\n",
    "\n",
    "df = pd.read_csv(input_dir + 'SERS_training.csv' , header = 0)\n",
    "combine = df.loc[:, 400.0:'Label'].to_numpy()\n",
    "np.random.seed(8787)\n",
    "np.random.shuffle(combine)\n",
    "feature = combine[:, :-1]\n",
    "label =  combine[:, -1]\n",
    "feature_train = combine[:, :-1]\n",
    "label_train =  combine[:, -1]\n",
    "train_size = int(feature.shape[0] * 1)\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(input_dir + 'SERS_testing.csv', header = 0)\n",
    "combine = df_test.loc[:, 400.0:'Label'].to_numpy()\n",
    "np.random.seed(8787)\n",
    "np.random.shuffle(combine)\n",
    "feature_test = combine[:, :-1]\n",
    "label_test =  combine[:, -1]\n",
    "test_size = int(feature_test.shape[0] * 1)\n",
    "\n",
    "\n",
    "\n",
    "#Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 6, 3, 2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 3, 2)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc1 = nn.Linear(2288, 280)\n",
    "        self.fc2 = nn.Linear(280, 14)\n",
    "        self.fc3 = nn.Linear(14, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0],1,-1))\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "feature_train = torch.from_numpy(feature_train)\n",
    "label_train = torch.from_numpy(label_train)\n",
    "feature_test = torch.from_numpy(feature_test)\n",
    "label_test = torch.from_numpy(label_test)\n",
    "\n",
    "train_dataset = Data.TensorDataset(feature_train, label_train)\n",
    "test_dataset = Data.TensorDataset(feature_test, label_test)\n",
    "train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "#Model Setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU run\")\n",
    "cnn_model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "\n",
    "\n",
    "#Model Training\n",
    "cnn_model_path = input_dir + 'SERS_CNN.pth'\n",
    "\n",
    "\n",
    "accuracy_record = {'train': [], 'test': []} \n",
    "loss_record = {'train': [], 'test': []} \n",
    "best_train_acc = 0.0\n",
    "best_train_loss = 0.0\n",
    "\n",
    "\n",
    "initial_time = time()\n",
    "\n",
    "for epoch in range(Epoch):  # loop over the dataset multiple times\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    cnn_model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss =  criterion(outputs, labels)\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "    accuracy_record['train'].append(train_acc/len(train_dataset))\n",
    "    loss_record['train'].append(train_loss/len(train_loader))    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:    # print every 2000 mini-batches\n",
    "        print(f'{epoch + 1}, train_loss: {train_loss /len(train_loader)}, train_acc: {train_acc/len(train_dataset)}')\n",
    "\n",
    "    if train_acc > best_train_acc:\n",
    "        best_train_acc = train_acc\n",
    "        print('[Save]-- [{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, Epoch, train_acc/len(train_dataset), train_loss/len(train_loader)\n",
    "            ))\n",
    "\n",
    "\n",
    "    cnn_model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = cnn_model(inputs)\n",
    "            loss =  criterion(outputs, labels)\n",
    "            _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "            test_acc += (test_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        accuracy_record['test'].append(test_acc/len(test_dataset))\n",
    "        loss_record['test'].append(test_loss/len(test_loader))\n",
    "\n",
    "\n",
    "\n",
    "torch.save(cnn_model.state_dict(), cnn_model_path)\n",
    "\n",
    "print('Finished Training (02)')\n",
    "print('Training time', time() - initial_time)\n",
    "\n",
    "\n",
    "acc_pd = pd.DataFrame.from_dict(accuracy_record)\n",
    "loss_pd = pd.DataFrame.from_dict(loss_record)\n",
    "lc_pd = df = pd.concat([acc_pd,loss_pd], axis=1)\n",
    "lc_filename =  input_dir + 'SERS_CNN_learnCurve.csv'\n",
    "lc_pd.to_csv(lc_filename, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML prediction: RF, SVM, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir + 'SERS_testing.csv', header = 0)\n",
    "# Sorting by Label\n",
    "df.sort_values(by = 'Label', inplace=True)\n",
    "feature = df.loc[:, 400.0:1550.0]\n",
    "\n",
    "\n",
    "label = df['Label'].to_numpy()\n",
    "label = torch.from_numpy(label)\n",
    "\n",
    "#Random Forest\n",
    "rf_result = rf.predict(feature)\n",
    "rf_result = torch.from_numpy(rf_result)\n",
    "\n",
    "#SVM\n",
    "svm_result = svm.predict(feature)\n",
    "svm_result = torch.from_numpy(svm_result)\n",
    "\n",
    "#KNN\n",
    "knn_result = knn.predict(feature)\n",
    "knn_result = torch.from_numpy(knn_result)\n",
    "\n",
    "#CNN\n",
    "cnn_result = df['Prediction'].to_numpy()\n",
    "cnn_result = torch.from_numpy(cnn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML prediction: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "df = pd.read_csv(input_dir + 'SERS_testing.csv', header = 0)\n",
    "feature = df.loc[:, 400.0:1550.0].to_numpy()\n",
    "feature = torch.from_numpy(feature)\n",
    "label = df['Label'].to_numpy()\n",
    "label = torch.from_numpy(label)\n",
    "\n",
    "\n",
    "#Data_Loader\n",
    "batch_num = feature.shape[0]\n",
    "dataset = Data.TensorDataset(feature, label)\n",
    "test_loader = Data.DataLoader(dataset, batch_size=batch_num)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 6, 3, 2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 3, 2)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc1 = nn.Linear(2288, 280)\n",
    "        self.fc2 = nn.Linear(280, 14)\n",
    "        self.fc3 = nn.Linear(14, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0],1,-1))\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(input_dir + 'SERS_CNN.pth'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred_acc = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        features, labels = data\n",
    "        features = features.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        features = features.reshape((features.shape[0],1, 1, -1))\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(features)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred_acc += (predicted.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "        cnn_result = predicted.to('cpu')\n",
    "\n",
    "\n",
    "cnn_result = cnn_result.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_calculation(predict, label):\n",
    "    correct = (predict == label).sum().item()\n",
    "    total = label.size(0) \n",
    "    return round(100 * correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RF\n",
    "rf_acc = acc_calculation(rf_result, label)\n",
    "\n",
    "#SVM\n",
    "svm_acc = acc_calculation(svm_result, label)\n",
    "\n",
    "#KNN\n",
    "knn_acc = acc_calculation(knn_result, label)\n",
    "\n",
    "#CNN\n",
    "cnn_acc = acc_calculation(cnn_result, label)\n",
    "\n",
    "\n",
    "print(f'Random Forest ACC: {rf_acc}% \\n SVM ACC: {svm_acc}% \\n KNN ACC: {knn_acc}% \\n CNN ACC: {cnn_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "xlabel_General = np.array([ 'BW25113','BW25113_Uninhibited','BW25113_Inhibited','DH5\\u03B1', 'DH5\\u03B1(WT)_Uninhibited', 'DH5\\u03B1(WT)_Inhibited', 'DH5\\u03B1(ampR)_Uninhibited'])\n",
    "x_General = np.array([0, 1, 2, 3, 4, 5, 6])\n",
    "color_map = [ 'powderblue', 'steelblue', 'navy', '#F6BE00','firebrick', 'maroon',  'olivedrab']\n",
    "marker_list = ['v', 'v', 'v',  'o', '8', '8', 'p' ]\n",
    "\n",
    "#RF\n",
    "plt.figure(1)\n",
    "rf_con = confusion_matrix(label, rf_result,normalize='true')\n",
    "rf_con = np.around(rf_con, 2)\n",
    "rf_disp = ConfusionMatrixDisplay(confusion_matrix=rf_con, display_labels= xlabel_General)\n",
    "rf_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(input_dir + 'RF_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "#SVM\n",
    "plt.figure(2)\n",
    "svm_con = confusion_matrix(label, svm_result,normalize='true')\n",
    "svm_con = np.around(svm_con, 2)\n",
    "svm_disp = ConfusionMatrixDisplay(confusion_matrix=svm_con,display_labels= xlabel_General)\n",
    "svm_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(input_dir + 'SVM_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "#KNN\n",
    "plt.figure(3)\n",
    "knn_con = confusion_matrix(label, knn_result,normalize='true')\n",
    "knn_con = np.around(knn_con, 2)\n",
    "knn_disp = ConfusionMatrixDisplay(confusion_matrix=knn_con, display_labels= xlabel_General)\n",
    "knn_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(input_dir + 'KNN_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "#CNN\n",
    "plt.figure(4)\n",
    "cnn_con = confusion_matrix(label, cnn_result, normalize='true')\n",
    "cnn_con = np.around(cnn_con,2)\n",
    "cnn_disp = ConfusionMatrixDisplay(confusion_matrix=cnn_con, display_labels= xlabel_General)\n",
    "cnn_disp.plot(cmap ='gist_yarg',  colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(input_dir + 'CNN_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "899abd191a9ea73a89afb19849dc406f29b830785484ec2528e59ce4ca659ec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
